{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320234bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d62f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "url = \"https://www.amazon.in/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "user_inp = input('Enter the product you want to search : ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e471d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_bar = driver.find_element(By.ID,\"twotabsearchtextbox\")   \n",
    "search_bar.clear()                                               \n",
    "search_bar.send_keys(user_inp)                                   \n",
    "search_button = driver.find_element(By.XPATH,'//div[@class=\"nav-search-submit nav-sprite\"]/span/input')     \n",
    "search_button.click()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef7f763",
   "metadata": {},
   "source": [
    "In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then scrape all the products available under that product name. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67442952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Empty list to scrap data\n",
    "Brand =[]\n",
    "Product = []\n",
    "Rating =[]\n",
    "No_of_ratings =[]\n",
    "Price =[]\n",
    "Return =[]\n",
    "Excepted_delivery =[]\n",
    "Availability =[]\n",
    "Other_details =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af953411",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL= []\n",
    "# range(0,3) used to scrape three pages on website\n",
    "for page in range(0,3):\n",
    "    url=driver.find_elements_by_xpath('//a[@class=\"a-link-normal a-text-normal\"]')\n",
    "    for i in url:\n",
    "        URL.append(i.get_attribute('href'))\n",
    "    time.sleep(2)\n",
    "    # locating next page button and clicking\n",
    "    Nxt_page=driver.find_element_by_xpath('//li[@class=\"a-last\"][1]/a')\n",
    "    Nxt_page.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dd56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193bd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(URL):\n",
    "    driver.get(i)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Extracting Brand Name via Xpath\n",
    "    try:\n",
    "        brand=driver.find_element_by_xpath('//a[@id=\"bylineInfo\"]')\n",
    "        Brand.append(brand.text) \n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "        \n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting product name via Xpath\n",
    "    try:\n",
    "        product =driver.find_element_by_xpath('//span[@id=\"productTitle\"]')\n",
    "        Product.append(product.text)\n",
    "    except NoSuchElementException:\n",
    "        Product.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Rating via Xpath\n",
    "    try: rating=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-base\"]')\n",
    "        Rating.append(rating.text)\n",
    "    except NoSuchElementException:\n",
    "        Rating.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting No of Ratings via Xpath\n",
    "    try:\n",
    "        rating_count=driver.find_element_by_xpath('//span[@class=\"a-size-base a-color-secondary\"]')\n",
    "        No_of_ratings.append(rating_count.text)\n",
    "    except NoSuchElementException:\n",
    "        No_of_ratings.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Price via Xpath\n",
    "    try:\n",
    "        price=driver.find_element_by_xpath('//span[@id=\"priceblock_dealprice\"]')\n",
    "        Price.append(price.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Return or exchange detail via Xpath\n",
    "    try:\n",
    "        replacement=driver.find_element_by_xpath('//*[@id=\"RETURNS_POLICY\"]/span/div[2]/a')\n",
    "        Return.append(replacement.text)\n",
    "    except NoSuchElementException:\n",
    "        Return.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting Expected Delivery via Xpath\n",
    "    try:\n",
    "        delivery=driver.find_element_by_xpath('//div[@id=\"ddmDeliveryMessage\"]/b')\n",
    "        Excepted_delivery.append(delivery.text)\n",
    "    except NoSuchElementException:\n",
    "        Excepted_delivery.append('-')\n",
    "    time.sleep(1)\n",
    "      # Extracting Availability via Xath\n",
    "    try:\n",
    "        availability=driver.find_element_by_xpath('//span[@class=\"a-size-medium a-color-success\"]')\n",
    "        Availability.append(availability.text)\n",
    "    except NoSuchElementException:\n",
    "        Availability.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Extracting other details via Xpath\n",
    "    try:\n",
    "        other=driver.find_element_by_xpath('//ul[@class=\"a-unordered-list a-vertical a-spacing-mini\"]')\n",
    "        Other_details.append(other.text)\n",
    "    except NoSuchElementException:\n",
    "        Other_details.append('-')\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cacc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Brand),len(ReturnExchange),len(Delivery),len(product),len(price),len(availability),len(Ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e08e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({'Brand':Brand,'ReturnExchange':ReturnExchange,'Price':price,'Delivery_by':Delivery,'urls':url,'Ratings':Ratings,'Name of Product':product,'Availability of product':availability})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496081b",
   "metadata": {},
   "source": [
    "Write a python program to access the search bar and search button on images.google.com and scrape 10 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c81fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome('chromedriver.exe')\n",
    "time.sleep(3)\n",
    "\n",
    "url = \"https://images.google.com/\"\n",
    "driver.get(url)\n",
    "search_bar = driver.find_element(By.XPATH,'//input[@class=\"gLFyf\"]')    \n",
    "search_bar.send_keys(\"fruits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9cb7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button = driver.find_element(By.XPATH,'//div[@class=\"zgAlFc\"]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "\n",
    "img_urls = []\n",
    "img_data = []\n",
    "for image in images:\n",
    "    source= image.get_attribute('src')\n",
    "    if source is not None:\n",
    "        if(source[0:4] == 'http'):\n",
    "            img_urls.append(source)\n",
    "            \n",
    "for i in range(len(img_urls)):\n",
    "    if i >= 100:\n",
    "        break\n",
    "    print(\"Downloading {0} of {1} images\" .format(i, 100))\n",
    "    response= requests.get(img_urls[i])\n",
    "    file = open(r\"C:\\Users\\Infinity\\Fliprobbo\\WebScraping Assignment 3 Selenium\\Cars\" +str(i)+ \".jpg\", \"wb\")\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606fc002",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032095f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver4=webdriver.Chrome('chromedriver.exe')\n",
    "driver4.get(\"https://www.flipkart.com/\")\n",
    "time.sleep(2)\n",
    "driver4.maximize_window()\n",
    "pop_close = driver4.find_element(\"xpath\",\"/html/body/div[2]/div/div/button\").click()\n",
    "product = input(\" Enter the name of Smartphone that has to be searched : \")\n",
    "search_product4=driver4.find_element(By.CLASS_NAME,'_3704LK').send_keys(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86573f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button4= driver4.find_element(By.CLASS_NAME,'L0Z3Pu').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "page1_urls = []\n",
    "urls = driver4.find_elements(By.XPATH,'//a[@class=\"_1fQZEK\"]')\n",
    "for url in urls:\n",
    "    page1_urls.append(url.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf55fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = []\n",
    "Phone_name = []\n",
    "Colour = []\n",
    "RAM= []\n",
    "ROM = []\n",
    "Primary_Camera= []\n",
    "Secondary_Camera = []\n",
    "Display_Size= []\n",
    "Battery_Capacity = []\n",
    "Price = []\n",
    "URL = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d805bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in page1_urls:\n",
    "    driver4.get(url)              \n",
    "    URL.append(url)                                                          \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    #Clicking on read more button\n",
    "    try:\n",
    "        read_more = driver4.find_element(By.XPATH,'//button[@class=\"_2KpZ6l _1FH0tX\"]')     \n",
    "        read_more.click()\n",
    "    except NoSuchElementException:\n",
    "        print(\"Exception occured while moving to next page\")\n",
    "        \n",
    "    try:\n",
    "        brand_tags = driver4.find_element(By.XPATH,'//span[@class=\"B_NuCI\"]')      \n",
    "        Brand.append(brand_tags.text.split()[0])\n",
    "    except NoSuchElementException:\n",
    "        Brand.append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping phone name data\n",
    "    try:\n",
    "        name_tags = driver4.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[3]/td[2]/ul/li')     \n",
    "        Phone_name.append(name_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Phone_name.append('-')\n",
    "    \n",
    "    \n",
    "    #Scraping phone color data\n",
    "    try:\n",
    "        color_tags = driver4.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][1]/table/tbody/tr[4]/td[2]/ul/li')      \n",
    "        Colour.append(color_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Colour.append('-')\n",
    "     \n",
    "    \n",
    "    #Scraping RAM data\n",
    "    try:\n",
    "        ram_tags = driver4.find_element(By.XPATH,'//div[@class=\"_3k-BhJ\"][4]/table[1]/tbody/tr[2]/td[2]/ul/li')                \n",
    "        RAM.append(ram_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        RAM.append('-')\n",
    "        try:\n",
    "        price_tags = driver4.find_element(By.XPATH,'//div[@class=\"_30jeq3 _16Jk6d\"]')      \n",
    "        Price.append(price_tags.text)\n",
    "    except NoSuchElementException:\n",
    "        Price.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a8ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "smartphone=pd.DataFrame({\"brand_name\":Brand[:10],\"smartphone_name\":Phone_name[:10],\"product_colour\":Colour[:10],\"product_RAM\":RAM[:10],\n",
    "                         \"product_ROM\":ROM[:10],\"Primary_Camera\":Primary_Camera[:10],\"Secondary_Camera\":Secondary_Camera[:10],\n",
    "                         \"Display_size\":Display_Size[:10],\"Battery_Capacity\":Battery_Capacity[:10],\"Phone_Price\":Price[:10],\"Product_URL\":URL[:10]})\n",
    "\n",
    "smartphone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0031812",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d7b672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e84a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url='https://www.google.co.in/maps'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aebf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element_by_id(\"searchboxinput\") \n",
    "# clearing any previous input in search bar\n",
    "Search.clear()\n",
    "# Feeding input specified by user to search menu through send keys\n",
    "Search.send_keys('Nashik')\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element_by_id(\"searchbox-searchbutton\")  \n",
    "# Clicking search button\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc577f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_url=driver.current_url\n",
    "print('Current url :',current_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f052d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if \"@\" in current_url:\n",
    "        location=current_url.split('@')[1].split(',*/data')[0].split(',')\n",
    "        location\n",
    "        print('Latitude of given Location:',location[0])\n",
    "        print('Longitude of given Location:',location[1])\n",
    "except:\n",
    "    print('Location detail not found in url')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d7cd10",
   "metadata": {},
   "source": [
    "6. Write a program to scrap details of all the funding deals for second quarter (i.e. Jan 21 – March 21) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf82d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url='https://www.trak.in'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be08a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Date=[]\n",
    "Startup=[]\n",
    "Industry=[]\n",
    "Sub_vertical=[]\n",
    "City=[]\n",
    "Investor=[]\n",
    "Investment_type=[]\n",
    "Amount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(48,51)):\n",
    "    # Extracting Date from table\n",
    "    try:\n",
    "        date=driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "        for j in date:\n",
    "            Date.append(j.text)\n",
    "    except:\n",
    "        Date.append('NA')\n",
    "        \n",
    "    # Extracting startup name via xpath\n",
    "    try:\n",
    "        startup=driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "        for j in startup:\n",
    "            Startup.append(j.text)\n",
    "    except:\n",
    "        Startup.append('NA')\n",
    "    \n",
    "    # Extracting Industry vertical via Xpathtry:\n",
    "        industry=driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "        for j in industry:\n",
    "            Industry.append(j.text)\n",
    "    except:\n",
    "        Industry.append('NA')\n",
    "    \n",
    "    # Extracting subvertical via xpath\n",
    "    try:\n",
    "        subvertical=driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "        for j in subvertical:\n",
    "            Sub_vertical.append(j.text)\n",
    "    except:\n",
    "        Sub_vertical.append('NA')\n",
    "    \n",
    "    # Extracting city/location via xpath\n",
    "    try:\n",
    "        city=driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "        for j in city:\n",
    "            City.append(j.text)\n",
    "    except:\n",
    "        City.append('NA')\n",
    "        \n",
    "    # Extracting Investor via xpath\n",
    "    try:\n",
    "        investor=driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "        for j in investor:\n",
    "            Investor.append(j.text)\n",
    "    except:\n",
    "        Investor.append('NA')\n",
    "        \n",
    "    # Extracting Investment type via xpath\n",
    "    try:\n",
    "        investment_type=driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "        for j in investment_type:\n",
    "            Investment_type.append(j.text)\n",
    "            except:\n",
    "        Investment_type.append('NA')\n",
    "        \n",
    "    # Extracting Amount type via xpath\n",
    "    try:\n",
    "        amount=driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "        for j in amount:\n",
    "            Amount.append(j.text)\n",
    "    except:\n",
    "        Amount.append('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Funding=pd.DataFrame({'Date':Date,'Startup Company':Startup,'Industry/Vertical':Industry,'Sub-Vertical':Sub_vertical,'City':City,\n",
    "                        'Investor':Investor,'Investment Type':Investment_type,'Amount':Amount})\n",
    "print('\\033[1m'+'Funding deals in second quarter (i.e. Jan 21 – March 21) :'+'\\033[0m')\n",
    "Funding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005bbbdc",
   "metadata": {},
   "source": [
    "7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c0447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url='https://www.digit.in'\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11921938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on top 10 option button in menu bar\n",
    "driver.find_element_by_xpath('//div[@class=\"menu\"]/ul/li[4]/a').click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Clicking on Laptop under table\n",
    "driver.find_element_by_xpath('//div[@class=\"categoty_list\"]/button[2]').click()\n",
    "\n",
    "# Opening Best gaming laptops link\n",
    "Gaming_lappy=driver.find_element_by_xpath('//div[@id=\"laptops\"]/div[3]/a').get_attribute('href')\n",
    "driver.get(Gaming_lappy)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Empty list for scraping data\n",
    "Laptop_model =[]\n",
    "OS =[]\n",
    "Display = []\n",
    "Processor =[]\n",
    "Memory =[]\n",
    "Weight =[]\n",
    "Dimension =[]\n",
    "Graphics_processor =[]\n",
    "Price =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dbfe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Laptop model via Xpath\n",
    "for i in range(0,11):\n",
    "    laptop_model=driver.find_elements_by_xpath('//*[@id=\"toptenIdevent{}\"]/a/h3'.format(i))\n",
    "    for j in laptop_model:\n",
    "       Laptop_model.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting OS on Laptop model via Xpath\n",
    "os=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[3]/td[3]')\n",
    "for j in os:\n",
    "    OS.append(j.text)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Display vertical via Xpath\n",
    "display=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[4]/td[3]')\n",
    "for j in display:\n",
    "    Display.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e478e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Processor via xpath\n",
    "processor=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[5]/td[3]')\n",
    "for j in processor:\n",
    "    Processor.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e8df53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Memory via xpath\n",
    "memory=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[6]/td[3]')\n",
    "for j in memory:\n",
    "    Memory.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f25940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Laptop Weight via xpath\n",
    "weight=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[7]/td[3]')\n",
    "for j in weight:\n",
    "    Weight.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c88f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Dimension via xpath\n",
    "dimension=driver.find_elements_by_xpath('//div[@class=\"Spcs-details\"]/table/tbody/tr[8]/td[3]')\n",
    "for j in dimension:\n",
    "    Dimension.append(j.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfab61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make data frame\n",
    "Gaming_Laptops=pd.DataFrame({\"Laptop Model\":Laptop_model,\"Price\":Price, \"OS\":OS,\"Display\":Display,\"Memory\":Memory,\"Processor\":Processor,\n",
    "                 \"Weight\":Weight,\"Dimension\":Dimension,\"Graphical processor\":Graphics_processor})\n",
    "print('\\033[1m'+'Best Gaming Laptop :'+'\\033[0m')\n",
    "Gaming_Laptops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8501a27f",
   "metadata": {},
   "source": [
    "8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25869e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.forbes.com\"\n",
    "driver.get(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f10f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on explore button to expand menu\n",
    "driver.find_element_by_xpath('//button[@class=\"icon--hamburger\"]').click()\n",
    "\n",
    "# Clicking on Billionaire category\n",
    "driver.find_element_by_xpath('//ul[@class=\"header__channels\"]/li[1]').click()\n",
    "\n",
    "# Clicking on world billionaire tab\n",
    "driver.find_element_by_xpath('//ul[@class=\"header__channels\"]/li[1]/div[2]/ul/li[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb61f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to scrap data\n",
    "Rank =[]\n",
    "Name=[]\n",
    "Net_worth=[]\n",
    "Age=[]\n",
    "Citizenship=[]\n",
    "Source=[]\n",
    "Industry=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Rank No\n",
    "rank=driver.find_elements_by_xpath('//div[@class=\"rank\"]')\n",
    "for i in rank:\n",
    "    Rank.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extacting name\n",
    "name=driver.find_elements_by_xpath('//div[@class=\"personName\"]/div')\n",
    "for i in name:\n",
    "    Name.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8040b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Net worth\n",
    "net=driver.find_elements_by_xpath('//div[@class=\"netWorth\"]/div')\n",
    "for i in net:\n",
    "    Net_worth.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b160c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Age\n",
    "age=driver.find_elements_by_xpath('//div[@class=\"age\"]/div')\n",
    "for i in age:\n",
    "    Age.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c320b06",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Extracting Citizenship\n",
    "citizen=driver.find_elements_by_xpath('//div[@class=\"countryOfCitizenship\"]')\n",
    "for i in citizen:\n",
    "    Citizenship.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fab656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Source\n",
    "try:\n",
    "    source=driver.find_elements_by_xpath('//div[@class=\"source-column\"]')\n",
    "    for i in source:\n",
    "        Source.append(i.text)\n",
    "except:\n",
    "    source=driver.find_elements_by_xpath('//*[@id=\"jeff-bezos\"]/div[6]/div/div[1]')\n",
    "    Source.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8adfdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Industry\n",
    "industry=driver.find_elements_by_xpath(\"//div[@class='category']//div\")\n",
    "for i in industry:\n",
    "    Industry.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870299c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Forbes_list=pd.DataFrame({'Rank':Rank,'Name':Name,'Net Worth':Net_worth,'Age':Age,'Citizenship':Citizenship,\n",
    "                'Source':Source,'Industry':Industry})\n",
    "print('\\033[1m'+' Forbes World Billionaires List Oct 2021 :'+'\\033[0m')\n",
    "Forbes_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d0dbf",
   "metadata": {},
   "source": [
    "9.Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411e455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.youtube.com\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8dc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding search menu by xpath\n",
    "Search=driver.find_element_by_xpath('//input[@id=\"search\"]') \n",
    "# Feeding input video name by user to search menu through send keys\n",
    "Search.send_keys('Healing Ragas - Sitar Tabla - Brindavan Sarang - Classical Instrumental Fusion B.Sivaramakrishna Rao')\n",
    "# Finding Search button for clicking through xpath\n",
    "Search_button=driver.find_element_by_xpath('//button[@id=\"search-icon-legacy\"]/yt-icon')  \n",
    "# Clicking search button\n",
    "Search_button.click()\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41a54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clicking on video\n",
    "link = driver.find_element_by_xpath(\"//yt-formatted-string[@class ='style-scope ytd-video-renderer']\")\n",
    "link.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f5494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrolling window using ScrollBy method from 0 pixel to 15000 pixel\n",
    "driver.execute_script(\"window.scrollBy(0,1000000)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17f99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty lists to scrap data\n",
    "Comments = []\n",
    "Comment_posted_ago = []\n",
    "Timeline = []\n",
    "Likes = []\n",
    "No_of_Likes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75592151",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting comment on video\n",
    "comment = driver.find_elements_by_id(\"content-text\")\n",
    "time.sleep(3)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(comment):\n",
    "    if i.text is None:\n",
    "        Comments.append(\"--\")\n",
    "    else:\n",
    "        Comments.append(i.text)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40bc578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting time of commenting\n",
    "timeline = driver.find_elements_by_xpath(\"//a[contains(text(),'ago')]\")\n",
    "time.sleep(3)\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(timeline):\n",
    "    if i.text is None:\n",
    "        Timeline.append(\"-\")\n",
    "    else:\n",
    "        Timeline.append(i.text)\n",
    "for i in tqdm(range(0,len(Timeline),2)):\n",
    "    Comment_posted_ago.append(Timeline[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape the comment likes\n",
    "like = driver.find_elements_by_xpath(\"//span[@class='style-scope ytd-comment-action-buttons-renderer']\")\n",
    "for i in like:\n",
    "    Likes.append(i.text)\n",
    "    \n",
    "for i in range(1,len(Likes),2):\n",
    "    No_of_Likes.append(Likes[i])\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23404d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Comments),len(Comment_posted_ago),len(No_of_Likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f3f239",
   "metadata": {},
   "outputs": [],
   "source": [
    "YT=pd.DataFrame({})\n",
    "YT['Comments']=Comments[:500]\n",
    "YT['Comment Posted Ago']=Comment_posted_ago[:500]\n",
    "YT['No. of Likes']=No_of_Likes[:500]\n",
    "YT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e357bb7",
   "metadata": {},
   "source": [
    "10.Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to webdriver\n",
    "driver=webdriver.Chrome(r'C:/chromedriver.exe')\n",
    "time.sleep(1)\n",
    "\n",
    "# Opening url in chrome browser\n",
    "url=\"https://www.hostelworld.com\"\n",
    "driver.get(url)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1efe0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_xpath('//*[@id=\"page-footer-accomodation\"]/ul/li[2]/a').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2075263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#locating the location search bar\n",
    "Search = driver.find_element_by_xpath('//input[@id=\"home-search-keywords\"]')\n",
    "\n",
    "# Sending input London in search bar\n",
    "Search.send_keys(\"London\")\n",
    "time.sleep(1)\n",
    "\n",
    "#select london\n",
    "london = driver.find_element_by_xpath('//*[@id=\"top-search\"]/div/div[1]/div[2]/ul/li[2]')\n",
    "london.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# do click on search button\n",
    "Search_button = driver.find_element_by_xpath('//*[@id=\"top-search\"]/div/div[2]/button')\n",
    "Search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets find required data\n",
    "hostel_name = []\n",
    "distance = []\n",
    "pvt_prices = []\n",
    "dorms_price = []\n",
    "rating = []\n",
    "reviews = []\n",
    "over_all = []\n",
    "facilities = []\n",
    "description =[]\n",
    "product_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376ffda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in driver.find_elements_by_xpath(\"//div[@class = 'pagination-item pagination-current' or @class='pagination-item']\"):\n",
    "    i.click()\n",
    "    time.sleep(4)\n",
    "    \n",
    "    #fetching hostel name\n",
    "    try:\n",
    "        name = driver.find_elements_by_xpath(\"//h2[@class='title title-6']\")\n",
    "        for i in name:\n",
    "            hostel_name.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        hostel_name.append('-')\n",
    "        \n",
    "    #fetching distance from city centre\n",
    "    \n",
    "    try:\n",
    "        dist = driver.find_elements_by_xpath(\"//div[@class='subtitle body-3']//a//span[1]\")\n",
    "        for i in dist:\n",
    "            distance.append(i.text.replace('Hostel - ',''))\n",
    "    except NoSuchElementException:\n",
    "        distance.append('-')\n",
    "        \n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "    #fetch privates from price\n",
    "        try:\n",
    "            pvt_price = driver.find_element_by_xpath(\"//a[@class='prices']//div[1]//div\")\n",
    "            pvt_prices.append(pvt_price.text)\n",
    "        except NoSuchElementException:\n",
    "            pvt_prices.append('-')\n",
    "    #fetching dorms from price\n",
    "    for i in driver.find_elements_by_xpath(\"//div[@class='prices-col']\"):\n",
    "        try:\n",
    "            dorms = driver.find_element_by_xpath(\"//a[@class='prices']//div[2]//div\")\n",
    "            dorms_price.append(dorms.text)\n",
    "        except NoSuchElementException:\n",
    "            dorms_price.append('-')\n",
    "    #fetching facilities\n",
    "    try:\n",
    "        fac1 = driver.find_elements_by_xpath(\"//div[@class='has-wifi']\")\n",
    "        fac2 = driver.find_elements_by_xpath(\"//div[@class='has-sanitation']\")\n",
    "        for i in fac1:\n",
    "            for j in fac2:\n",
    "                facilities.append(i.text +', '+ j.text )\n",
    "    except NoSuchElementException:\n",
    "        facilities.append('-')\n",
    "    #lets fetch url of each hostel\n",
    "    p_url = driver.find_elements_by_xpath(\"//div[@class='prices-col']//a[2]\")\n",
    "    for i in p_url:\n",
    "        product_url.append(i.get_attribute('href'))\n",
    "\n",
    "for i in product_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    #lets click on show more button for description\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"//a[@class='toggle-content']\").click()\n",
    "        time.sleep(5)\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    #fetching ratings\n",
    "    try:\n",
    "        rat = driver.find_element_by_xpath(\"//div[@class='score orange big' or @class='score gray big']\")\n",
    "        rating.append(rat.text)\n",
    "    except NoSuchElementException:\n",
    "        rating.append('-')\n",
    "    #fetching total reviews\n",
    "         try:\n",
    "        rws = driver.find_element_by_xpath(\"//div[@class='reviews']\")\n",
    "        reviews.append(rws.text.replace('Total Reviews',''))\n",
    "    except NoSuchElementException:\n",
    "        reviews.append('-')\n",
    "    #fetch overall review\n",
    "    try:\n",
    "        overall_rw = driver.find_element_by_xpath(\"//div[@class='keyword']//span\")\n",
    "        over_all.append(overall_rw.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')\n",
    "    #fetch property description \n",
    "    try:\n",
    "        disc = driver.find_element_by_xpath(\"//div[@class='content']\")\n",
    "        description.append(disc.text)\n",
    "    except NoSuchElementException:\n",
    "        over_all.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a3d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dataframe\n",
    "data = list(zip(hostel_name,distance,rating,reviews,over_all,pvt_prices,dorms_price,facilities,description))       \n",
    "Hostel = pd.DataFrame(data, columns = [\"Hostel name\",\"Distance from city centre\",\"ratings\",\"Total reviews\",\"Overall review\",\"Privates from price\",\"Dorms from price\",\"Facilities\",\"Property Description\"])\n",
    "print('\\033[1m'+' Hostel Available in London :'+'\\033[0m')\n",
    "Hostel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60251e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
